#!/usr/bin/env python3
"""
Test API Fallback Functionality
æµ‹è¯•Cerebras -> OpenAI APIå›é€€æœºåˆ¶
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.llm_client import LLMClient
from src.features.impostor_game.service import ImpostorGameService

def test_llm_client_fallback():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯çš„å›é€€åŠŸèƒ½"""
    print("ğŸ”„ Testing LLM Client Fallback Functionality\n")
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = LLMClient()
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Say hello in a friendly way."}
    ]
    
    print("1. Testing normal LLM generation...")
    try:
        response = llm_client.generate_response(test_messages, max_tokens=50, temperature=0.7)
        print(f"   âœ… Response: {response}")
        print(f"   ğŸ“Š Usage stats: {llm_client.get_usage_stats()}")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    return llm_client

def test_roundtable_with_fallback():
    """æµ‹è¯•Round Tableä¼šè®®ç³»ç»Ÿçš„APIå›é€€åŠŸèƒ½"""
    print("\nğŸ¯ Testing Round Table with API Fallback\n")
    
    # åˆ›å»ºæœåŠ¡
    service = ImpostorGameService()
    
    print("1. Creating game...")
    init_response = service.create_game()
    game_id = init_response.game_id
    
    print(f"   Game ID: {game_id}")
    print(f"   Impostor: {init_response.impostor_revealed}")
    print(f"   Reporter: {init_response.reporter_name}")
    
    print("\n2. Running Round Table meeting (with potential fallback)...")
    try:
        step_response = service.step_game(game_id)
        
        if step_response:
            print(f"   âœ… Meeting completed successfully!")
            print(f"   Statements: {len(step_response.statements)}")
            print(f"   Votes: {len(step_response.votes)}")
            print(f"   Eliminated: {step_response.eliminated or 'None'}")
            print(f"   Game over: {step_response.game_over}")
            
            # æ˜¾ç¤ºAPIä½¿ç”¨ç»Ÿè®¡
            llm_stats = service.llm_client.get_usage_stats()
            print(f"\n   ğŸ“Š API Usage Statistics:")
            print(f"     Cerebras calls: {llm_stats['cerebras_calls']}")
            print(f"     OpenAI calls: {llm_stats['openai_calls']}")
            print(f"     Rate limit hits: {llm_stats['rate_limit_hits']}")
            print(f"     Total calls: {llm_stats['total_calls']}")
            
            if llm_stats['openai_calls'] > 0:
                print(f"   ğŸ”„ Fallback was used! OpenAI handled {llm_stats['openai_calls']} calls")
            else:
                print(f"   âœ… Cerebras handled all calls successfully")
            
            # æ˜¾ç¤ºéƒ¨åˆ†å¯¹è¯
            print(f"\n   ğŸ’¬ Sample dialogue:")
            for i, stmt in enumerate(step_response.statements[:3]):
                follow_up = " (follow-up)" if stmt.is_follow_up else ""
                print(f"     {stmt.agent_name}{follow_up}: {stmt.content}")
            
            return True
        else:
            print("   âŒ Step response was None")
            return False
            
    except Exception as e:
        print(f"   âŒ Error during meeting: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_multiple_meetings():
    """æµ‹è¯•å¤šè½®ä¼šè®®ä»¥éªŒè¯æŒç»­çš„APIå›é€€åŠŸèƒ½"""
    print("\nğŸ” Testing Multiple Meetings for Sustained Fallback\n")
    
    service = ImpostorGameService()
    init_response = service.create_game()
    game_id = init_response.game_id
    
    meeting_count = 0
    max_meetings = 3  # é™åˆ¶æµ‹è¯•è½®æ•°
    
    while meeting_count < max_meetings:
        meeting_count += 1
        print(f"Meeting {meeting_count}:")
        
        try:
            step_response = service.step_game(game_id)
            
            if step_response:
                stats = service.llm_client.get_usage_stats()
                print(f"   âœ… Completed - Cerebras: {stats['cerebras_calls']}, OpenAI: {stats['openai_calls']}")
                
                if step_response.game_over:
                    print(f"   ğŸ Game ended: {step_response.winner} wins!")
                    break
            else:
                print(f"   âŒ Failed to get step response")
                break
                
        except Exception as e:
            print(f"   âŒ Error: {str(e)[:100]}...")
            break
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = service.llm_client.get_usage_stats()
    print(f"\nğŸ“Š Final API Usage Statistics:")
    print(f"   Total meetings: {meeting_count}")
    print(f"   Cerebras calls: {final_stats['cerebras_calls']}")
    print(f"   OpenAI calls: {final_stats['openai_calls']}")
    print(f"   Rate limit hits: {final_stats['rate_limit_hits']}")
    print(f"   Success rate: {((final_stats['total_calls'] - final_stats['rate_limit_hits']) / max(final_stats['total_calls'], 1) * 100):.1f}%")

if __name__ == "__main__":
    try:
        print("=" * 60)
        print("ğŸš€ API Fallback Testing Suite")
        print("=" * 60)
        
        # æµ‹è¯•åŸºæœ¬LLMå®¢æˆ·ç«¯
        llm_client = test_llm_client_fallback()
        
        # æµ‹è¯•Round Tableä¼šè®®
        success = test_roundtable_with_fallback()
        
        if success:
            # æµ‹è¯•å¤šè½®ä¼šè®®
            test_multiple_meetings()
        
        print("\n" + "=" * 60)
        print("âœ… API Fallback Testing Complete!")
        print("ğŸ’¡ The system will automatically use OpenAI when Cerebras hits rate limits")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Test suite failed: {e}")
        import traceback
        traceback.print_exc()
